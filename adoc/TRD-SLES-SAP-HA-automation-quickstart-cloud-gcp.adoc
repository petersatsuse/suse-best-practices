:docinfo: 

// Set variable for Cloud.  This can be "Azure","AWS" or "GCP"
//:cloud: Azure
:cloud: GCP
:sles: SUSE Linux Enterprise Server
:sles4sap: {sles} for SAP Applications

= Using SUSE Automation to Deploy an SAP HANA Cluster on Google Cloud Platform: Getting Started


== About the guide

This document will walk you through the deployment of a simple two-node SAP HANA HA Cluster using the SUSE SAP Automation Project and operating on Google Cloud Platform (GCP).
This project uses Terraform and Salt to deploy and configure the operating system (SUSE Linux Enterprise Server for SAP Applications), SAP software (SAP HANA), and a SUSE Linux Enterprise High Availability (HA) cluster.
If extensive configuration and customization are required, refer to the project documentation at https://github.com/SUSE/ha-sap-terraform-deployments.

For simplicity, this guide uses the Cloud Shell to perform the deployment, as it provides easy access to most of the required tooling.

It is possible to easily use a local Linux or macOS computer, but some commands may need modification or omission.

The architecture for the deployment is similar to the below:

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
image::TRD_SLES-SAP-HA-automation-quickstart-cloud-gcp-automation-architecture.png[title=GCP Automation Architecture,scaledwidth=99%]
endif::[]

The project will perform the following actions:

* Deploying infrastructure - including Virtual Private Network (VPC), subnet, firewall rules etc.
* Deploying instances - 2x SAP HANA Instances
* Configuring the operating system for SAP workload
* Running the SAP HANA installation
* Configuring SAP HANA System Replication (HSR)
* Configuring SUSE Linux Enterprise High Availability cluster components and resources


== Configuring the Cloud Shell

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]

IMPORTANT: The GCP user specified in the below-mentioned step needs certain GCP Project permissions to ensure the deployment is successful. 
For simplification, the GCP user used in this guide has the _Project Owner_ IAM role. 

The following procedures show the minimum steps required to prepare the GCP infrastructure to host the SAP HANA environment:

. Create a new GCP Project to host the SAP HANA environment.
. Enable the GCP Compute Engine API.
. Using the newly created GCP project console, start a GCP Cloud Shell.
. Using the GCP Cloud Shell, create a new GCP Key for the default GCP Service Account. The key will be used by Terraform to access the GCP infrastructure. 
+
TIP: For more details about creating GCP Service Account key, refer to https://cloud.google.com/iam/docs/creating-managing-service-account-keys#iam-service-account-keys-create-gcloud.
+
NOTE: For simplification, the default GCP Service Account will be used in this guide.
endif::[]


== Ensuring Terraform is installed

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
Terraform is already deployed as part of the GCP Cloud Shell. The following command output shows the Terraform version used at the time of creating this guide:
----
$ terraform -v
Terraform v1.0.0
on linux_amd64
----
endif::[]


== Preparing the SAP HANA media

The SAP HANA Media needs to be made available so it can be accessed during the deployment.
The SUSE SAP Automation project allows for three methods for presenting the SAP media:

a. SAR file and SAPCAR executable (SAP HANA Database only)
b. Multipart exe/RAR files
c. Extracted media

This guide recommends the simplest method which is the extracted media. With the correct entitlement, SAP HANA media can be downloaded from the SAP Web site at
https://support.sap.com/en/my-support/software-downloads.html.

This guide uses the most recent SAP HANA media, SAP HANA 2.0 SPS05. The SAP HANA media file name downloaded at the time of creating this guide is `51054623.ZIP`.  
Follow the SAP instructions to extract the SAP HANA media. For illustration, the final extracted directory name is `51054623`.

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
A GCP Cloud Storage bucket is used to host the SAP HANA extracted media. Using the GCP Console, perform the following actions:

* Create a new GCP bucket. (The example shows a GCP Cloud Storage bucket called `mysapmedia`, but a unique name should be used.)
* Upload the SAP HANA media extracted directory to the GCP Cloud Storage bucket. The following figure shows the uploaded SAP HANA media extracted directory:
+
image::trd_sles-sap-ha-automation-quickstart-cloud-gcp-bucket.png[title=SAP HANA GCP Storage Bucket, scaledwidth=99%]
endif::[]



== Downloading and configuring the SUSE Automation code

The SUSE SAP Automation code is published in GitHub. The following commands will:

. Create a new GCP Cloud Shell directory to host the SUSE SAP automation code
. Change directory to the newly created directory
. Clone the project to the Cloud Shell ready for configuration

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
$ mkdir suse-sap-automation
$ cd suse-sap-automation
$ git clone --branch 7.2.0 https://github.com/SUSE/ha-sap-terraform-deployments.git
----
endif::[]

Next, move the generated GCP Service Account Key to the SUSE SAP Automation GCP directory:
----
$ cd ~
$ cp <GCP Service Account Key> suse-sap-automation/ha-sap-terraform-deployments/gcp
----

NOTE: If SSH keys already exist, the next step can be skipped.

++++
<?pdfpagebreak?>
++++

Then, generate SSH key pairs to allow for accessing the GCP SAP HANA instances:
----
#optional if ssh-keys already exist
$ cd ~
$ ssh-keygen -q -t rsa -N '' -f  ~/.ssh/id_rsa
----


=== Configuring the deployment options and modifying the Terraform variables

The files that need to be configured are contained in a subdirectory of the project. Use that as the working directory:

ifeval::[ "{cloud}" == "Azure" ]
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
$ cd ~/suse-sap-automation/ha-sap-terraform-deployments/gcp
----
endif::[]

A Terraform template is provided. For a demo environment, this will need around 13 parameters modifying.
Rename the Terraform example file to `terraform.tfvars`:
----
$ mv terraform.tfvars.example terraform.tfvars
----

Edit the `terraform.tfvars` file and modify it as explained below.
If you are duplicating the lines before modification, ensure the original is commented out, or the deployment will fail.

ifeval::[ "{cloud}" == "GCP" ]
First, choose the GCP Project ID for the deployment:
----
# GCP project id
project = "<PROJECT ID>"
----

Then, choose the GCP Service Account Key file path. With the following parameter, Terraform will use the GCP Service Account Key file created above:
----
# Credentials file for GCP
gcp_credentials_file = "<GCP Service Account Key Path and Name>"
----
endif::[]


ifeval::[ "{cloud}" == "Azure" ]
Choose the region for the deployment:
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
Choose the region for the deployment:
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
Choose the GCP region for the deployment:
----
# Region where to deploy the configuration
region = "<GCP Region Name>"
----
endif::[]

The following parameters select the version of SUSE Linux Enterprise Server for SAP Applications to deploy:

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
os_image = "suse-sap-cloud/sles-15-sp2-sap"
----
endif::[]

Next, enter the path for the public and private SSH keys that were generated earlier.

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
# SSH Public key location to configure access to the remote instances
public_key  = "~/.ssh/id_rsa.pub"

# Private SSH Key location
private_key = "~/.ssh/id_rsa"
----
endif::[]

To keep the cluster architecture and deployment simple and to provide additional packages needed to deploy, uncomment and set the following parameters:

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
ha_sap_deployment_repo = "https://download.opensuse.org/repositories/network:/ha-clustering:/sap-deployments:/v7/"
----

Then, enable the `pre_deployment` parameter:
----
pre_deployment = true
----

Disable the Bastion Host Server creation parameter `bastion_enabled`:
----
bastion_enabled = false
----
endif::[]

Next, set which SAP HANA GCP instance machine type should be selected:

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
machine_type = "n1-highmem-32"
----
endif::[]


Modify the following parameter to point to SAP media that was uploaded to the storage location:

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
----
hana_inst_master = "mysapmedia/51054623"
----
endif::[]

Finally, to ensure a fully automated deployment, it is possible to set passwords within the `terraform.tfvars` file. Uncomment and set the following parameters:
----
hana_master_password = "SAP_Pass123"
netweaver_master_password = "SAP_Pass123"
----

NOTE: If the parameters are not set in the `terraform.tfvars` file, they must be entered when running the deployment.

IMPORTANT: All passwords must conform to SAP password policies or the deployment will fail.


Optional: If a monitoring instance is required as part of the deployment, find and uncomment the following:

----
monitoring_enabled = true
----


== Finalizing the automation configuration

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
Ensure that the GCP Project used to host the SAP HANA HA cluster meets the infrastructure quota requirements set by Google Cloud. For more info, refer to https://cloud.google.com/solutions/sap/docs/sap-hana-planning-guide#quotas
endif::[]


== Deploying the project

Terraform will create and name resources on GCP when running the deployment based on the "workspace" in use.
It is highly recommended to create a unique workspace from which to run the deployment.

----
$ terraform init
$ terraform workspace new demo
$ terraform workspace select demo
$ terraform plan
$ terraform apply
----

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
TIP: The GCP Cloud Shell has a timeout of around 20 minutes. After that period, the shell session will be closed if left
unattended, resulting in a failed deployment. It is strongly advised to retain focus on the GCP Cloud Shell window to ensure the timeout does not occur.
endif::[]

If successful, the output lists the public IP addresses for the cluster nodes. This will look similar to the following considering the different Public IP addresses for each deployment:
-----
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Summary for local
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): -------------
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Succeeded: 33 (changed=23)
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Failed:     0
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): -------------
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Total states run:     33
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Total run time: 1028.670 s
module.hana_node.module.hana_provision.null_resource.provision[1] (remote-exec): Wed Jun 23 10:14:22 UTC 2021::demo-hana02::[INFO] deployment done 
module.hana_node.module.hana_provision.null_resource.provision[1]: Creation complete after 27m24s [id=3463680564647535989]

Apply complete! Resources: 26 added, 0 changed, 0 destroyed.

Outputs:

bastion_public_ip = ""
cluster_nodes_ip = [
  "10.0.0.10",
  "10.0.0.11",
]
cluster_nodes_name = [
  "demo-hana01",
  "demo-hana02",
]
cluster_nodes_public_ip = tolist([
  "34.127.16.75",
  "34.145.94.26",
])
cluster_nodes_public_name = []
drbd_ip = []
drbd_name = []
drbd_public_ip = []
drbd_public_name = []
iscsisrv_ip = ""
iscsisrv_name = ""
iscsisrv_public_ip = ""
iscsisrv_public_name = []
monitoring_ip = ""
monitoring_name = ""
monitoring_public_ip = ""
monitoring_public_name = ""
netweaver_ip = []
netweaver_name = []
netweaver_public_ip = []
netweaver_public_name = []
-----


== Tearing down

When finished with the deployment, or even if the deployment has failed, ensure that Terraform is used to tear down the environment.

----
$ terraform destroy
----

ifeval::[ "{cloud}" == "Azure" ]
AZURE
endif::[]

ifeval::[ "{cloud}" == "AWS" ]
AWS
endif::[]

ifeval::[ "{cloud}" == "GCP" ]
This method will ensure all GCP resources, such as instances, disks, VPCs, and roles are cleaned up. You need to delete the following GCP components manually: 

* GCP Cloud Storage bucket
* GCP Project
endif::[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
== Legal notice
include::common_sbp_legal_notice.adoc[]

++++
<?pdfpagebreak?>
++++

// Standard SUSE Best Practices includes
// :leveloffset: 0
include::common_gfdl1.2_i.adoc[]
